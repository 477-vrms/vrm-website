<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
    <base href="../../" />

    <!--Content-->
    <title>ECE477 Course Documents</title>
    <meta name="keywords" content="" />
    <meta name="description" content="" />
    <meta name="author" content="George Hadley">
    <meta name ="format-detection" content = "telephone=no" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">

    <!--CSS-->
    <link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
    <link rel="stylesheet" href="css/responsive.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/content.css">
    <link rel="stylesheet" href="css/progress.css"/>
    <!--[if IE 6]>
    <link href="default_ie6.css" rel="stylesheet" type="text/css" />
    <![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
        <!-- Instantiate global site header.-->
        <div id="header"></div>
        <!-- Instantiate site global navigation bar.-->
        <div id="menu"></div>

        <!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
            img folder located at this directory level. -->
        <div id="banner">
            <img src="Files/img/BannerImgExample.jpg"/>
        </div>

        <!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
            and include things such as a file lister (for listing out homework assignments or tutorials)
        -->
        <div id="content">
            <h2>Progress Report for Matthew Wen</h2>

            <br>
            <div style="border: solid #1F1F1F; width: fit-content; padding: 5px;">
                <p style="display: block">Table of Contents</p>
                <ul style="display: flex; ">
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week2-div">Week 2</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week3-div">Week 3</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week4-div">Week 4</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week5-div">Week 5</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week6-div">Week 6</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week7-div">Week 7</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week9-div">Week 9</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week11-div">Week 11</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week12-div">Week 12</a></li>
                    <li style="margin: 10px;"><a href="Team/progress/wen101.html#week13-div">Week 13</a></li>
                </ul>
            </div>

            <div id="week2-div">
                <h4>Week 2:</h4>
                <b>Date:</b> Jan 20, 2022<br>
                <b>Total hours:</b> 30 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals for the Week</p>
                <p>My main goals for the week is to setup up tools and environments for the team to use software wise. This includes an API backend that can be easily updated with no issue, and then getting some sort of connection setup between Brian’s already working unity code, and some API route I created in the backend. </p>

                <p class="p-header">Server setup</p>
                <p>For the server, we concluded to use Google Cloud. The reason being is that AWS (Amazon Web Servers) relies on the members creating new accounts with Amazon, and Amazon EC2 Free Tier ended up being more expensive for the long run compared to Google Cloud VM Instance. Amazon EC2 and Google Cloud VM are basically Computer in the Cloud that can handle either hosting a website, or an API Gateway. Our idea is to setup an API Gateway with our Google Cloud VM instance so all our components (VR Headset, and Raspberry PI).</p>

                <p class="p-header">DevOps Continuous Integration and Deployment with API Gateway</p>
                <p>We currently have this
                    <a href="https://github.com/477-vrms/vrms-api">https://github.com/477-vrms/vrms-api</a>
                    repository. It is a similar replica to the ECESS API gateway
                    <a href="https://github.com/Purdue-ECESS/ecess-api">https://github.com/Purdue-ECESS/ecess-api</a>
                    . The API from ECESS is developed by both me and James. Basically how it works, for every commit made to the main branch, GitHub builds and packages a docker image of our source code. After it packages the source code, it gains access to our Google Cloud Account using WIF (Workload Identity Pools):
                    <a href="https://cloud.google.com/blog/products/identity-security/enabling-keyless-authentication-from-github-actions">https://cloud.google.com/blog/products/identity-security/enabling-keyless-authentication-from-github-actions</a>.
                </p>

                <img src="Files/img/wen101/week2/google_wif.png"/>

                <p>
                    Basically, in this tutorial, it uses a service account, in this case, [gh-actions-to-vm@mwenclubhouse.iam.gserviceaccount.com], to authenticate with GitHub. In the screenshot from Google Cloud from above, I say that if the GitHub repository is from 477-vrms/vrms-api, then it can access to my Google Cloud Account.
                    GitHub uses this authenticate to gain access to my VM instance, and then therefore delete the old image of our source code, and replace it with the new image create from GitHub. Here is an example.
                    <a href="https://github.com/477-vrms/vrms-api/runs/4872020682?check_suite_focus=true">https://github.com/477-vrms/vrms-api/runs/4872020682?check_suite_focus=true</a>
                </p>

                <img src="Files/img/wen101/week2/gh_actions.png"/>

                <p>
                    From the first few tasks (Set up job, Checkout repository, Log in to the Container registry, Extract Metadata (tags, labels) for Docker and Build and Push to Docker image), GitHub is using its program to download our source code, and then run the build command to create an image of our image with our source code. For the next few tasks (Set up Cloud SDK), it obtain access to my Google Cloud Account, in this case “mwenclubhouse”, and obtains access to the Computer by telling Google to Generate an ssh key to login into the instance. The next step (Use GCP CLI), it runs commands directly on the instance.  For the last few steps (Post Setup Cloud SDK, Post Auth with GCP with WIF, etc), it delete the ssh key generated through this process to ensure nobody can access the instance if that key information is leaked, and then clean up building the docker container, and all other information.
                </p>

                <p>
                    One of the major reasons of doing this is that it is already setup with James and mine participation in ECESS, and we know how simple it is to setup, so we set it up to improve efficiency. In addition, it allows the team to keep the production code separate from development in any case anything breaks. If the server was a place for development, then if Brian was testing something with the server, it is going to cause roadblocks.  Lastly, Docker images retains the setup on how a program is being built, so we don’t have to worry there is a screw up between our development version compared to our production version.
                </p>

                <p>
                    Brian and I actually struggled with this. We had Web Sockets not being able to transmit data to the server with a snippet of unity code we found online. We didn’t know what was the issue. As a result, we pulled aside and ensure that it worked locally so we can crack down on the issue; long story short, we were not sure at all what the issue was, but after doing a minor change, we were able to get it working. I was worried thinking that it will hopefully worked with <a href="https://ecess-api.matthewwen.com/vrms">https://ecess-api.matthewwen.com/vrms</a> (our api gateway), but nope, it ran perfectly fine because we ensure out development version run as close to what matches in production as possible.
                </p>


                <p class="p-header">Server Setup</p>
                <p>
                    On the server, as mentioned in the previous section, we are using Docker Containers. Here is our current configuration of how our Docker Container runs.
                </p>
                <div style="border: groove;">
                    <code>
                        sudo docker run -d --env-file /home/mwenclubhouse/environments/vrms.env.list \ <br>
                        <span style="margin-left: 50px;"></span>-v /home/mwenclubhouse/vrms:/usr/src/bucket \ <br>
                        <span style="margin-left: 50px;"></span>--name vrms-api \ <br>
                        <span style="margin-left: 50px;"></span>--restart always \ <br>
                        <span style="margin-left: 50px;"></span>-p 2000:8000 \ <br>
                        <span style="margin-left: 50px;"></span>-p 2001:8001 \ <br>
                        <span style="margin-left: 50px;"></span>ghcr.io/477-vrms/vrms-api:main"
                    </code>
                </div>

                <p>
                    We ensure that the container we have running will run on restart, mostly because if Google decides to restart our computer, the program will automatically start up when the computer boots up. We also link port 8000 of the container to port 2000 on the Google VM, and same applies to port 8001 and 2001. Port 8001 is not used for anything at the moment besides
                </p>
                <p>
                    To deal with routing, we are using nginx; we don’t need a load balance, but it is really nice to have nginx deal with routing with our program.
                </p>
                <div style="border: groove;">
                    <code>
                        <span>location /vrms/ { </span><br>
                        <span style="margin-left: 30px;"></span>proxy_pass http://127.0.0.1:2000/; <br>
                        <span style="margin-left: 30px;"></span>proxy_http_version 1.1; <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Upgrade $http_upgrade; <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Connection "Upgrade";  <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Host $host; <br>
                        } <br>
                    </code>
                </div>

                <p>
                    In this case, we have our docker container running on port 2000 map to
                    <a href="https://ecess-api.matthewwen.com/vrms">https://ecess-api.matthewwen.com/vrms</a>
                    . As a result,
                    <a href="https://ecess-api.matthewwen.com/vrms/route">https://ecess-api.matthewwen.com/vrms/route</a>
                    links to
                    <a href="http://127.0.0.1:2000/route">http://127.0.0.1:2000/route</a> locally on the Google VM. You can see the API is online by going into
                    <a href="https://ecess-api.matthewwen.com/vrms/">https://ecess-api.matthewwen.com/vrms/</a>.
                </p>
                <img src="Files/img/wen101/week2/gateway_demo.png"/>

                <p class="p-header">Programming Language Used for API Application</p>
                <p>
                    So for the backend, we are currently using express-ws, which is a combination of both express as a REST API, and Web-Sockets.
                    <a href="https://github.com/HenningM/express-ws">https://github.com/HenningM/express-ws</a>
                    . Our programming language of choice is Typescript because it performs type checking on top of javascript, so typescript can catch errors before I run into errors. On top of javascript, we are using node to run out javascript code; node is a JIT compiler that converts javascript code into machine code, which then therefore can run on our server.
                </p>

                <p class="p-header">
                    Benchmarking Unit Web Socket
                </p>

                <p>
                    As a software engineering lead, I’m mostly in control on ensuring that all our components can communicate with each other. To reiterate the components is the Unity Software, which runs on the Virtual Reality Headset, the Server, and the IoT device. Using Brian’s Unity code, Brian had an idea to be able to upload joint data from unity to the server. In addition, we also wanted to test how fast it sends data from the server. As a result, I decided to use a Google Firebase Account, where we can use a real time Firebase database so we can easily see the data point changing. The reason on why we decided to Google Firebase because Google created an intuitive software where we can visibly see the changes. In addition, the reason of why we use Google Real Time Database then Google Firestone is because Google Real Time bills us based off the number of bytes sent, not based off of the number of requests, which is another database option offered in Firebase. Keep in mind, we are only using Firebase to check that data is being received, not as an actual service.
                </p>
                <img src="Files/img/wen101/week2/firebase.png"/>
                <p>
                    The structure is not final, as a result, if Brian needs to adjust the data to label them as more specific joints, he can adjust the data that is being posted to the Web Socket.
                    For the Web Socket, the data being sent to the server is in json format. So for this example, it is being sent like this
                </p>

                <div class="div-flex">
                    <div style="border: groove;">
                        <code>
                            { <br>
                            <span style="margin-left: 30px;"></span>"Joint 1": -13.28888, <br>
                            <span style="margin-left: 30px;"></span>"Joint 2": 5.666, <br>
                            <span style="margin-left: 30px;"></span>"Joint 3": 9.42342 <br>
                            }
                        </code>
                    </div>
                    <div style="flex: 1; margin: 5px">
                        <p>
                            At this point of the project, we were not really concern about the data being useful, mostly that the there is a connection between the VR Unity Application and the Server. The server confirms that it obtained data from the VR headset by processing sent via JSON format and then putting it into points→person inside the database. We quickly learn that it updates really fast, even considering the fact on how fast Google Firebase Application runs via Google Servers and on the Computer we are observing the data on. However, we do notice we might be sending data a little too fast, therefore for the later weeks, we need to consider the frequency on how much data to send because sending too much data is just bad and could potentially overload the server and the robot arm badly.
                        </p>
                    </div>

                </div>


                <p class="p-header">What I learned</p>
                <p>
                    Throughout this entire process, I learned about how service accounts work on Google Cloud, and how to connect a Google Cloud Account to a GitHub repository. In addition, I learned how to deploy an API gateway with a Docker Container with the help of using the template from ECE 468
                    <a href="https://github.com/ECE468/env-container-2021-fall">https://github.com/ECE468/env-container-2021-fall</a>
                    , but we did it with node instead of antlr for them. Lastly, I learned how to deploy a web-socket application with my Google Computer (Google VM). In addition, I connected the web-socket application to both the VR headset and a database just to showcase.
                </p>

                <p class="p-header">Next Steps</p>
                <p>The original idea is to use Web Sockets for video streaming since there isn’t really a limit on how much data can be sent in a Web Socket. We also want to implement MQTT, which is also a tcp protocol that should send data faster in smaller packets since the MQTT format has a limit on the amount of data can be sent per packet. We are going to use MQTT for sending data from the unity headset to the robot.</p>

            </div>
            <hr>
            <div id="week3-div">
                <h4>Week 3:</h4>
                <b>Date:</b> Jan 27, 2022<br>
                <b>Total hours:</b> 15 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals of the Week</p>
                <p>
                    My main goal this week was to connect the raspberry pi to the server. This is an important step
                    because we can finally have a proposed pipeline where a unity project (software that runs on the VR
                    headset) can fully connect to the raspberry pi.
                </p>
                <p class="p-header">Setting Up the PI</p>
                <p>
                    The first step is to set up the PI. The raspberry pi will be using Raspbian, the official server
                    side software to run on the PI. We don't need a full GUI to run programs on the PI. The software
                    can be found here <a href="https://www.raspberrypi.com/software/">https://www.raspberrypi.com/software/</a>.
                    In addition, we decided to stick with Python as the software of choice on the PI because of other
                    team member experience with using Python on the PI; I have experience in both C and Python, but using C
                    Programming language could pop up new problems we don't want to run into in the future. At the moment,
                    the PI can successfully turn on inside the lab. The next part is deciding how our program will run
                    on it.
                </p>
                <p class="p-header">Using Systemctl to Run our Software</p>
                <p>
                    We can across <a href="https://en.wikipedia.org/wiki/Systemd">systemd</a>, which is a systemctl
                    client where it can start, stop, and reload programs in the background; you don't need a continuous
                    active terminal in order to run the program. Here is the idea on what we want to run:
                    <code>
                        <br>
                        $ sudo systemctl start vrms-pi # start our program <br>
                        $ sudo systemctl stop vrms-pi # stops our program <br>
                    </code>
                    In addition to being able to start and stop the program, you can also run environment variables etc.
                    This isn't set up yet since I want to write code that can connect to the server first before doing
                    anything else with the pi.
                    <img src="Files/img/wen101/week3/docker-systemd.jpg"/>
                </p>

                <p class="p-header">Setting up Message Broker with the Server and Issues</p>
                <div>
                    We tried using MQTT with our Google Cloud Server, but we realized and the team and TAs came into an
                    agreement to use UDP for most of our protocol instead of TCP. As a result, we decided to change
                    our focus to <a href="https://nodejs.org/api/dgram.html">dgram</a> to create udp websockets. In
                    addition, we initially wanted to use <a href="https://zeromq.org/">ZeroMQ</a> message broker, but
                    it doesn't have UDP support in its primary build; in other words, we have to do additional
                    configuration to enable it. As a result, we are going to use
                    <a href="https://www.rabbitmq.com/">RabbitMQ</a>. So far, I reconfigured the Google Cloud VM
                    to accept UDP traffic into it.
                    <div style="display: flex; margin-top: 10px">
                        <div>
                            <img height="300px" src="Files/img/wen101/week3/gcloud-network-config.jpg"/>
                            <br>
                            <img height="50px" src="Files/img/wen101/week3/gcloud-network-tag.jpg"/>
                        </div>
                        <div style="flex: 1">
                            I did this by first updating the network tag as shown below, which is a rule that states to open
                            all UDP ports.
                            In addition, attach the network tag to our Virtual Google Cloud Machine.
                        </div>
                    </div>
                </div>

                <p class="p-header">What I Learned</p>
                <p>
                    I learned a little about MQTT, which is now useless because we will be switching over to a
                    new type of message broker. In addition, I learned about differences in UDP vs TCP, and which
                    protocol to use; I learned I need to look more deeply into dgram and RTSP for video streaming.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    My next step is to do the connection with the raspberry pi with UDP with the server. We do need
                    to redo the steps for the joint data being sent to the server since that is done using TCP, but
                    we just want to at least have the full pipeline working; we will redo it another time. I need to
                    work close with Emma since she will also be doing a lot of connections between the pi and robotic arm.
                </p>
            </div>

            <div id="week4-div">
                <h4>Week 4:</h4>
                <b>Date:</b> Feb 4, 2022<br>
                <b>Total hours:</b> 20 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals of the Week</p>
                <p>
                    So my main goal of this week was to start having Brian and Emma communicate how to translate
                    data from unity to data on the arm. It seemed like Brian was moving very quickly with having
                    most of the unity configuration setup, and Emma already has the arm moving. As a result,
                    the next step was to link them where we can demo the arm being moved by Brian using Unity.
                </p>

                <p class="p-header">Reconfiguration of Google Cloud</p>
                <p>
                    So in addition of UDP being added to the Google VM, we also added TCP back into Google Cloud. After
                    looking more into <a href="https://www.rabbitmq.com/">Rabbit MQ</a>, we realized that RabbitMQ requires
                    a RabbitMQ server; we do realize that we can set one up, but our main goal was for us to code the
                    message broker part, so we reverted to <a href="https://zeromq.org/">ZeroMq</a>. We used the
                    <a href="https://github.com/zeromq/zeromq.js/">Node JS Library</a>, as well as the
                    <a href="https://github.com/zeromq/pyzmq">Python Library</a> for the Raspberry Pi. The Raspberry Pi
                    will keep a stable connection with the Google VM, and respond to instances when the server sends an
                    update.
                </p>

                <p class="p-header">Getting Data from Server to Pi</p>
                <p>
                    So we got data onto the pi in a json format. It takes the json from ZeroMQ and parse it into a
                    python object. For now, we put the joint data into a csv file. Here is a small snippet of the data.
                </p>
                <p>Python Program Running on Pi</p>
                <img src="Files/img/wen101/week4/pi-status.jpg"/>
                <p>logs.txt and joint.csv is the output</p>
                <img src="Files/img/wen101/week4/home-directory.jpg"/>
                <p>Content inside of joint.csv</p>
                <div style="overflow: scroll">
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.54'}, 1643926693.8405492, 3.300549268722534 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.61'}, 1643926693.844577, 3.234577178955078 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.67'}, 1643926693.892745, 3.222744941711426 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.75'}, 1643926693.930231, 3.1802310943603516 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.82'}, 1643926694.005648, 3.185647964477539 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.87'}, 1643926694.0465941, 3.1765942573547363 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.90'}, 1643926694.076011, 3.176010847091675 <br>
                </div>
                <p>
                    J1 ... J8 represents the joint data. T represent the epoch time in seconds - the first 4 digits. We kept
                    track on when we received the data, which is the data after the json packet, and then compare the difference.
                    We did this to compare the time it takes to go from unity to the pi, and we realized something seemed off
                    with the data. More will be explained later. However, this is a major success to have data from unity
                    finally show up on the Pi, where Emma can start taking that JSON data and have the arm react to it.
                </p>

                <p class="p-header">What I Learned</p>
                <p>
                    For one our PSSE is to ensure that packets are being sent in less than 0.5 seconds from unity to the
                    pi. We initially had the idea of timestamping the packet with when the packet is created, and then
                    calculating the difference between the epoch time on the pi and the timestamp on the packet.
                    We realized that the clock on both devices are not exactly the same, so we calculated that on
                    average, the time difference is 3 seconds, which is awful. So we used a phone and recorded the change
                    from the unity to the pi, and it was less than 0.5 seconds; we measured the difference in frames
                    by using the iPhone's slo-mo.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    So we got UDP setup, with some socket code on python. It is not running at the moment, but
                    it is on there. Emma got the camera for the pi, so the next step is using udp and sending the video
                    stream data to the server, and then server to website for debugging purposes. We don't know how
                    do it on unity, but we will figure that out on a later date.
                </p>
            </div>

            <div id="week5-div">
                <h4>Week 5:</h4>
                <b>Date:</b> Feb 11, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week, I've been mostly debugging my work because the Raspberry Pi ran into a few issues this week.
                    One of the issue was that it would run out of space because of the continuation of log messages; we
                    were able to fix that. In addition, we still needed to measure the time difference between data sent
                    from the unity headset and data received on the pi. Finally, I was reading up on documentation on
                    the camera we bought, but currently having trouble getting the pi to recognize the camera using the
                    default <a href="https://github.com/raspberrypi/libcamera-apps">libcamera-apps</a> from raspbian
                    themselves.
                </p>

                <p class="p-header">Pi Issues: Camera</p>
                <p>
                    We were able to upload code where the Pi should be able to continuously send data back and forth
                    with the robotic arm; the next step is to do it on command. As a result, I've been helping
                    Emma trying to set that up, so she understands how the data from the server is coming in and out.
                    As in terms of the camera, I'm still confused on how to set that up. My team members found me a
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">tutorial</a>,
                    so I'll use that as reference.
                </p>
                <img src="Files/img/wen101/week5/camera-connected.jpg" style="max-height: 200px"/>
                <img src="Files/img/wen101/week5/lib-camera.jpg" style="max-height: 200px"/>
                <p class="p-header">Pi Issues: SDCard</p>
                <p>
                    In addition, we had an issue with log messages. Since our code doesn't really do anything other than
                    receive data, we just print the data it receives. But it is receiving the data at such a high rate
                    that we ran out of space just to log messages. It was very annoying.
                    As a result, we used this website <a href="https://andreaskaris.github.io/blog/linux/setting-journalctl-limits/">tutorial</a>
                    to limit the amount of log messages to save on the disk. This prevented any errors where we just try to edit a file,
                    or install a new application, but is limited because the log messages took up 11 GB of the disk space.
                </p>
                <img src="Files/img/wen101/week5/fixing-issue.jpg"/>

                <p class="p-header">What I learned</p>
                <p>
                    I learned how to connect the camera to the pi hardware wise; I think I did it correctly because I
                    saw lights turn on, but when I tried to use libcamera to open the camera, it doesn't work. As a
                    result, I have to do some more digging around so then Brian can start playing around with it on
                    unity. According to this
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">website</a>,
                    I have to use VLC player; I'll try it next week.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    Basically getting the camera set up for Brian where the Pi can send data back to the server.
                    After that is complete, I can worry about video streaming data back to unity.
                    Probably going to be watching those PCB videos just learn a little more about PCB, but it
                    wouldn't be my main focus since Emma and James will be working on that mostly.
                </p>
            </div>

            <div id="week6-div">
                <h4>Week 6:</h4>
                <b>Date:</b> Feb 18, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week, I continued working on issues with the Pi, like receiving data from the server.
                    In addition, I worked on getting the camera setup, which is not really working on the
                    moment, more information about that later. Lastly, I was working on the Software Formation Document.
                </p>

                <p class="p-header">Pi Issues: Camera Setup and Getting Data from the Pi</p>
                <p>
                    One issue we ran into this week was not being able to see that the Pi got data from the server. It
                    was working before, but then we added some changes, and it wouldn't budge anymore. As a result,
                    because we used GitHub as part of our file tracking, we backtracked our commits until it began
                    working again; Brian set up unity to send random data, so basically I was looking to see those
                    random data from Brian, and then compared the difference. Apparently, the Pi didn't like the order
                    of declarations inside my background.py file compared to my Computer. The changes can be scene
                    <a href="https://github.com/477-vrms/vrms-pi/commit/5dd1ae0994b58bc8a3bb53fac6db1f1724c8b1ed">here</a>.
                </p>
                <img src="Files/img/wen101/week6/recieve-not-working.jpg"/>
                <p>
                    Another issue was the camera again. When following the
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">instructions</a>,
                    from before, it ran into kernel
                    not supported. So instead, I tried to find a version where it did support my kernel version on
                    the Pi; I was able to find
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/pivariety/how-to-install-kernel-driver-for-pivariety-camera/">this one</a>
                    , and it recognized the camera, but it noted that it was out of date,
                    so nothing will work. After even doing more research, I learned that "out of date" means
                    "not compatible". As a result, I've been trying to build it from source if that has any hope.
                    If this doesn't work, then I'll have to wipe the pi and downgrade it to a lower kernel version.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned some commands from
                    <a href="https://github.com/raspberrypi/libcamera-apps">libcamera-apps</a>
                    , and then some basics on how to obtain camera
                    data with
                    <a href="https://opencv.org/">opencv</a>
                    , according to documentation of the camera we are using, but I still have a long
                    way to go before actually sending data from the pi to the server.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    The next step is still unfortunately connect the camera to the pi and be able to take a picture.
                    Then, be able to send camera data to the server using UDP.
                </p>

            </div>

            <div id="week7-div">
                <h4>Week 7:</h4>
                <b>Date:</b> Feb 25, 2022<br>
                <b>Total hours:</b> 6 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week, I worked on debugging the raspberry pi, and tried to get the
                    camera working. In addition, I helped Emma debug the pi where she
                    can send data from the pi to the arm via serial.
                </p>

                <p class="p-header">Pi Issues: Camera Issue Stalled</p>
                <p>
                    I tried to get the camera drivers installed on the pi. All attempts failed because
                    the drivers are awful maintained by the developers, and the drivers they have available
                    for us to install were made for linux 32 bit; our pi is 64 bit. In addition, we just
                    realized our pi is 64 bit, so that why it was working so slow. Lastly, we were not able
                    to get the camera working at all this week. At the moment, we will be working around the problem,
                    but at the moment, we will buy the default raspberry pi camera on it because that will
                    definitively work. That camera will come on Wednesday from Amazon.
                </p>

                <p class="p-header">PI Issue: Wiping and Reworking</p>
                <p>
                    The Raspberry Pi screen froze because of the many failed attempts to install the drivers
                    for the camera. As a result, we wiped it with the 64 bit version OS for the raspberry pi
                    because the serial connection was not working with port 14 and 15 with the current OS. We
                    were able to get it working, and I helped the team connect the entire pipeline of the
                    VR headset moving, and the pi parsing the data from the VR headset.
                </p>
                <p>
                    I guess it is nice to know how to set up the pi easily when we do a fresh install.
                </p>

                <p class="p-header">Real Time Streaming Protocol</p>
                <p>
                    Because I was not able to get the camera set up, we are going to samples from this website,
                    <a href="https://file-examples.com/index.php/sample-video-files/sample-mp4-files/">https://file-examples.com/index.php/sample-video-files/sample-mp4-files/</a>
                </p>
                <img src="Files/img/wen101/week7/earth.jpg"/>
                <p>
                    It is a 30-second video we are going to put on repeat, and we are going to put it on repeat just
                    so we can get server to unity side working. We are going to use the
                    Real Time Streaming Protocol (RTSP) for video streaming; all it requires is port 554 to be open
                    and the link being rtsp://ecess-api.matthewwen.com/video/[id] where id will be the camera
                    to get the feed from. For default, it will just be 0. On the unity side, we are going to
                    see if there is an application that supports rtsp on the unity side. According to multiple
                    sources, they say you have to import the
                    <a href="https://code.videolan.org/videolan/vlc-unity">VLC media player</a> into the
                        application. However,
                    we are going to use this <a href="https://github.com/gujadot/RTSP_Unity_Plugin">repo</a>
                    as reference.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned that camera we got for our project, the
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/">
                        Arducam 2MP*2 Stereo Camera MIPI Module
                    </a>
                    is not a good. We tried contacting them for help to set it up on the Raspberry Pi 4 Model B.
                    We will be waiting for a response from them. As in terms of this camera, as long as we have at
                    least a camera, we will meet the requirements.
                </p>


                <p class="p-header">Next Steps</p>
                <p>
                    I'm going to have Brian work on the camera setup with the Pi because another viewpoint on
                    how to set up the camera helps. I'm going to work on the new camera when it comes in on
                    Wednesday. I'm also trying to get UDP RSTP all set up now that I'm off camera duty for a bit.
                </p>
            </div>

            <div id="week9-div">
                <h4>Week 9:</h4>
                <b>Date:</b> Mar 11, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week and last week, we worked on the camera, and setting up UDP to work with the Google Server
                </p>

                <p class="p-header">Server: Setting up UDP</p>
                <p>
                    I had coded that only work with one way connection. In other words, it was only working locally where
                    the pi can send data to the server, but the server cannot send messages back. As a result, I had to
                    restructure my code where it kept the ip address of the unity hardware, and then the ip address of
                    the raspberry pi. The purpose of this is, so then it can retain the address if it needs to send data
                    via UDP. After restricting the code, I was able to get it where the sever can send anything through
                    UDP dgram without the need for the pi or the headset to request data. At this point, we have the
                    pi send messages back and forth * n amount of times to send data. In addition, on the unity headset
                    side, it is able to send UDP messages to the server, and retrieve UDP data.
                </p>
                <img src="Files/img/wen101/week9/docker.jpeg"/>

                <p class="p-header">Pi: Camera</p>
                <p>
                    In lab today, huge thanks to James Donnelly, one of the best lab partners ever. Anyway, we were
                    able to at least get the default raspberry pi camera working with the pi. Here is an example picture.
                </p>
                <img src="Files/img/wen101/week9/pi.jpeg"/>

                <p class="p-header">Pi: Stereo Camera</p>
                <p>
                    The stereo camera been giving the team a lot of pain. At this point, we are going to send them what
                    we are doing, and then the error messages that we ran into in hope that we get our money worth out
                    of them. This process will probably continue during break as well.
                </p>

                <p class="p-header">Pi: Queue System</p>
                <p>
                    We have a queue system to recognize new requests from the pi. From my experience in OS (ECE 469),
                    I learned the difference between a thread and a process, so I converted all my python
                    process to thread because process basically fork the program, and we want the memory to be shared
                    not isolated,
                    so we are going to use Thread class in python.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned that I have to allow UDP through a docker container to use UDP. In addition, I learned
                    about some boot config. I'm still trying to learn more about stereo cameras. I also learned about
                    UDP and actually getting that part work. I was worried that wasn't going to work at all.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    The next steps would be to figure out the data type to pass through UDP. I'm not sure to do json
                    format to send each picture 30 frames per second, but I'll figure it out.
                </p>
            </div>

            <div id="week11-div">
                <h4>Week 11:</h4>
                <b>Date:</b> March 25, 2022<br>
                <b>Total hours:</b> 3 <br>
                <b>Description of design efforts:</b><br>
                <p class="p-header">Lab Synopsis</p>
                <p>
                    I continued contact with the stereoscopic camera to hopefully get it working. In addition,
                    I started to get the default pi camera connected to the camera, and I was working on connecting
                    it to the python code.
                </p>

                <p class="p-header">Pi: Worse Case Scenario</p>
                <p>
                    For the worst case, libcamera came with a script where it can do a UDP connection of the feed
                    to a server. I was planning on having that setup where if it's the raspberry pi, it just sends
                    the UDP over to the server directly.
                </p>
                <p>
                    Another solution would be to have the raspberry pi send the camera data continously to a random
                    local port, and then have the python code connect to that random port, and then transcribe and
                    update that UDP data to the cloud server.
                </p>

                <p class="p-header">Pi: Best Case Scenario</p>
                <p>
                    We initally looked into this python library,
                    <a href="https://github.com/waveform80/picamera.git">https://github.com/waveform80/picamera.git</a>.
                    The issue is that it doesn't support the raspberry pi 4 64 bit, so we have to use this
                    repository. So the following code below doesn't work.
                </p>
                <img src="Files/img/wen101/week11/code.jpg"/>
                <p>
                    <a href="https://github.com/raspberrypi/picamera2">https://github.com/raspberrypi/picamera2</a>
                    For this repository however, you still have to build from source, and when I did that, it broke
                    the pi a little, so I might need to try something else.
                </p>

                <p class="p-header">Pi: Camera Connection</p>
                <p>
                    When going back and forth about the stereoscopic camera, I got this response.
                </p>
                <img src="Files/img/wen101/week11/camera_support.jpg"/>
                <p>
                    So far, we are using the camera replacement from the lab, so we can return the new camera I bought
                    from Amazon, and then we are going to return the camera that caused us so much trouble from before.
                    The seller wants to just update the firmware and return it back, but we want a full on refund
                    because the trouble isn't worth it.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned about how to connect to the camera via python, and setting it up is probably not
                    as easy as I thought it would be. This week have been a little crazy, so I'll get this working
                    as soon as possible.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    I need to implement one of the ideas I just mentioned. I'm going to try the best case, and if I
                    don't have that working near the end of the week, I'll switch to the worst case. The main goal
                    is just to have it working.
                </p>
            </div>


            <div id="week12-div">
                <h4>Week 12:</h4>
                <b>Date:</b> April 1, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    I worked on what I mentioned last week, getting the code ready for the camera. So my main focus
                    was getting that UDP pipeline fully running now that the camera was hypothetically setup.
                </p>

                <p class="p-header">Pi: The Camera Again</p>
                <p>
                    We made the assumption throughout the week that the camera was working, but then one day
                    the camera just broke on us again. Based off all documentation we looked at, they all use
                    libcamera-apps as their source of getting data from the camera, and the script to install
                    libcamera-apps python library was failing.
                </p>
                <img src="Files/img/wen101/week12/no-camera.jpg"/>
                <p>
                    So as a result, I decided to just have the pi send
                    camera data to the port 4200, and then the python program binds to that port and sends that
                    data to google cloud. The reason of why we are doing this is because the build to access the
                    bytes directly from the libcamera library wasn't working. In addition, it messed up what we
                    initally had working.
                </p>
                <img src="Files/img/wen101/week12/build-fail.jpg"/>
                <p>
                    As of 8:21 pm from James Donnelly, he got the camera up and working. We had to reset the pi, and
                    then set up everything all over again. It has been such a pain, but it is so nice to have it working
                    again. Here is a picture of James using the camera <3.
                </p>
                <img src="Files/img/wen101/week12/james-goat.jpg"/>

                <p class="p-header">Pi: UDP Updates</p>
                <p>
                    I've been thinking a lot about how to structure this data. So for the initial request, it is
                    going to take a JSON bytes to send it to the server to store the pi UDP addresses. Afterwards,
                    it will retrieve the raw UDP bytes from the camera to the pi. The initial JSON bytes is for
                    authentication that it is from the raspberry pi, and the rest is just the camera feed.
                </p>

                <p class="p-header">Server: TCP Updates</p>
                <p>
                    Because we will be using UDP, and UDP needs a confirmation that it is connected or not to the
                    raspberry pi. As a result, we are going to have TCP do the confirmation for us that it
                    successfully retrieve the UDP address of the Pi and the VR headset.
                </p>

                <p class="p-header">Server: UDP Updates</p>
                <p>
                    So I reconfigured the Server to do the same thing. It will first have the unity server send an
                    initial UDP JSON object just so it has the address to the unity VR headset, and then have it
                    retrieve the bytes. While writing this, I realized that there isn't a confirmation that it
                    has both addresses because UDP doesn't check if the packet been successfully sent, so I will
                    have to integrate from UDP for do the confirmation that it has both the UDP address of the raspberry
                    pi and the VR headset.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned that Camera on the Raspberry Pi is annoying, and that I was really second guessing putting
                    UDP and TCP on this device. After working on it, I'm very thankful that I have both protocol onto
                    the device. It isn't possible to do all this with UDP, which I've been thinking for the entire
                    time. We do need some level handshake to ensure the connection is successful.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    I still need to use TCP to check if the connection between the raspberry pi, VR headset, and the
                    Google Server is successful. In addition, I need to get the unity code all up and running. And
                    since James got the Camera Code working, all I have to do is link everything up, and I should be
                    set.
                </p>

            </div>

            <div id="week13-div">
                <h4>Week 13:</h4>
                <b>Date:</b> April 8, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    Today, we worked on transferring the code from the pi4 to pi3. In addition, we got bytes to be sent
                    from the pi to the unity headset. The main goal is to finalize the pipeline for the video streaming,
                    so we can have a full working project.
                </p>

                <p class="p-header">Pi: Transfer from Pi4 to Pi3</p>
                <p>
                    The reason of why we switched from the pi4 to the pi3 because there is so much more support for
                    pi3 software, and the newest software for the pi4 doesn't work. The connection between the
                    pi4 and the default camera from raspbian does not work at all or
                    is just really buggy
                    , and I just found my own pi3.
                    As a result, we just set up the pi3. Emma going to be working on getting the current
                    code working on the pi3, however, we have the pi4 as backup if the pi3 does not work with
                    any of our PSSE. All the software from the pi4 with the connection with the server worked
                    right out of the box.
                </p>

                <p class="p-header">Server: Handle UDP connection</p>
                <p>
                    Within the TCP connection with the raspberry pi, it retrieves a command from TCP to connect the
                    raspberry pi UDP port to the server. For the full pipeline, the VR headset will connect its UDP port
                    with the server first. It will then trigger the raspberry pi via MQTT to tell it to connect
                    to the server with UDP. It will continue that process until the server tell the pi via UDP that
                    it successfully connected to the pi. Then the raspberry pi will send the camera data from the pi
                    to the VR headset no problem. Below is a video of it working, however the resolution is
                    bad because we have to send small packet sizes. As a result, we worked on an
                    algorithm that broke the image up into different packets, and then combine them in unity.
                </p>

                <video width="320" height="240" controls>
                    <source src="Files/img/wen101/week13/inital-udp.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

                <p class="p-header">VR Headset: Headset</p>
                <p>
                    The initial issue is that we do not understand the data from the stream. So we're currently
                    working on parsing that data and getting an image from it. We later discover it to be
                    sending h264 data fragments; how that worked is that it would send an initial frame being
                    what is currently in the frame. Afterwards, it would send the small changes that occur from
                    one frame to another frame. The issue is that unity doesn't have a library that allows us to
                    parse a h264 and convert it to an image, in addition that initial frame is really large. As a result,
                    we decided to stay with sending a continuous stream of jpeg images; maybe later, we will switch to
                    h264. However, since h264 and jpeg requires sending large packets, we decided it would be wise to
                    split the image up into different packets, and then combine them back together into one. The idea
                    came from
                    <a href="https://en.wikipedia.org/wiki/GStreamer">gstreamer</a> and from
                    <a href="https://engineering.purdue.edu/ece264/19sp/hw/HW11">Professor Quinn's BMP homework assignment</a>.
                    Basically, we added a header to each packet indicating the following. The raspberry pi would
                    split the image into multiple different packets, and then the unity code will switch the packets
                    into one image, and then show it front of a texture. We have it for jpeg, but this code will
                    also be useful if we decide to pivot and use h264.
                </p>
                <p class="p-header">
                    This is a photo on how we split it in python.
                </p>
                <img src="Files/img/wen101/week13/python.jpeg"/>
                <p class="p-header">
                    This is how the unity code parse the data from python and then stich them together.
                </p>
                <img src="Files/img/wen101/week13/unity.jpg"/>

                <p class="p-header">
                    This is it working all together. It probably a little more slow than before.
                </p>
                <video style="" width="320" height="240" controls>
                    <source src="Files/img/wen101/week13/udp-combine.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

                <p class="p-header">What I learned</p>
                <p>
                    I learned that the new libraries for the pi 4 is good but very buggy and unreliable, but
                    the pi 3 libraries is more reliable and not buggy. In addition, all the libraries for raspi
                    camera is working.
                </p>
                <p class="p-header">Next Steps</p>
                <p>
                    Because we have a setup with the pi 3, and we came to the conclusion that the pi 4 was the
                    problem the entire time, we decided to try the old camera again, in other words, the stereoscopic
                    camera. We think it will most likely work, and it will add more polish to our project.
                </p>
            </div>

            <!--            <div id="week3-div">-->
            <!--                <h4>Week 3:</h4>-->
            <!--                <b>Date:</b> Jan 27, 2022<br>-->
            <!--                <b>Total hours:</b> 10 <br>-->
            <!--                <b>Description of design efforts:</b><br>-->
            <!--                <p class="p-header">Lab Synopsis</p>-->
            <!--                <p class="p-header">What I learned</p>-->
            <!--                <p class="p-header">Next Steps</p>-->
            <!--            </div>-->
            <br>
        </div>

        <!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
        <div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
    $(document).ready(function() {
        $("#header").load("header.html");
        $("#menu").load("navbar.html");
        $("#footer").load("footer.html");
    });
</script>
</body>
</html>
