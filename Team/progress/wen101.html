<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
    <base href="../../" />

    <!--Content-->
    <title>ECE477 Course Documents</title>
    <meta name="keywords" content="" />
    <meta name="description" content="" />
    <meta name="author" content="George Hadley">
    <meta name ="format-detection" content = "telephone=no" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">

    <!--CSS-->
    <link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
    <link rel="stylesheet" href="css/responsive.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/content.css">
    <link rel="stylesheet" href="css/progress.css"/>
    <!--[if IE 6]>
    <link href="default_ie6.css" rel="stylesheet" type="text/css" />
    <![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
        <!-- Instantiate global site header.-->
        <div id="header"></div>
        <!-- Instantiate site global navigation bar.-->
        <div id="menu"></div>

        <!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
            img folder located at this directory level. -->
        <div id="banner">
            <img src="Files/img/BannerImgExample.jpg"/>
        </div>

        <!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
            and include things such as a file lister (for listing out homework assignments or tutorials)
        -->
        <div id="content">
            <h2>Progress Report for Matthew Wen</h2>

            <br>
            <div style="border: solid #1F1F1F; width: fit-content; padding: 5px;">
                <p style="display: block">Table of Contents</p>
                <ul style="display: block">
                    <li><a href="Team/progress/wen101.html#week2-div">Week 2</a></li>
                    <li><a href="Team/progress/wen101.html#week3-div">Week 3</a></li>
                    <li><a href="Team/progress/wen101.html#week4-div">Week 4</a></li>
                    <li><a href="Team/progress/wen101.html#week5-div">Week 5</a></li>
                    <li><a href="Team/progress/wen101.html#week6-div">Week 6</a></li>
                </ul>
            </div>

            <div id="week2-div">
                <h4>Week 2:</h4>
                <b>Date:</b> Jan 20, 2022<br>
                <b>Total hours:</b> 30 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals for the Week</p>
                <p>My main goals for the week is to setup up tools and environments for the team to use software wise. This includes an API backend that can be easily updated with no issue, and then getting some sort of connection setup between Brian’s already working unity code, and some API route I created in the backend. </p>

                <p class="p-header">Server setup</p>
                <p>For the server, we concluded to use Google Cloud. The reason being is that AWS (Amazon Web Servers) relies on the members creating new accounts with Amazon, and Amazon EC2 Free Tier ended up being more expensive for the long run compared to Google Cloud VM Instance. Amazon EC2 and Google Cloud VM are basically Computer in the Cloud that can handle either hosting a website, or an API Gateway. Our idea is to setup an API Gateway with our Google Cloud VM instance so all our components (VR Headset, and Raspberry PI).</p>

                <p class="p-header">DevOps Continuous Integration and Deployment with API Gateway</p>
                <p>We currently have this
                    <a href="https://github.com/477-vrms/vrms-api">https://github.com/477-vrms/vrms-api</a>
                    repository. It is a similar replica to the ECESS API gateway
                    <a href="https://github.com/Purdue-ECESS/ecess-api">https://github.com/Purdue-ECESS/ecess-api</a>
                    . The API from ECESS is developed by both me and James. Basically how it works, for every commit made to the main branch, GitHub builds and packages a docker image of our source code. After it packages the source code, it gains access to our Google Cloud Account using WIF (Workload Identity Pools):
                    <a href="https://cloud.google.com/blog/products/identity-security/enabling-keyless-authentication-from-github-actions">https://cloud.google.com/blog/products/identity-security/enabling-keyless-authentication-from-github-actions</a>.
                </p>

                <img src="Files/img/wen101/week2/google_wif.png"/>

                <p>
                    Basically, in this tutorial, it uses a service account, in this case, [gh-actions-to-vm@mwenclubhouse.iam.gserviceaccount.com], to authenticate with GitHub. In the screenshot from Google Cloud from above, I say that if the GitHub repository is from 477-vrms/vrms-api, then it can access to my Google Cloud Account.
                    GitHub uses this authenticate to gain access to my VM instance, and then therefore delete the old image of our source code, and replace it with the new image create from GitHub. Here is an example.
                    <a href="https://github.com/477-vrms/vrms-api/runs/4872020682?check_suite_focus=true">https://github.com/477-vrms/vrms-api/runs/4872020682?check_suite_focus=true</a>
                </p>

                <img src="Files/img/wen101/week2/gh_actions.png"/>

                <p>
                    From the first few tasks (Set up job, Checkout repository, Log in to the Container registry, Extract Metadata (tags, labels) for Docker and Build and Push to Docker image), GitHub is using its program to download our source code, and then run the build command to create an image of our image with our source code. For the next few tasks (Set up Cloud SDK), it obtain access to my Google Cloud Account, in this case “mwenclubhouse”, and obtains access to the Computer by telling Google to Generate an ssh key to login into the instance. The next step (Use GCP CLI), it runs commands directly on the instance.  For the last few steps (Post Setup Cloud SDK, Post Auth with GCP with WIF, etc), it delete the ssh key generated through this process to ensure nobody can access the instance if that key information is leaked, and then clean up building the docker container, and all other information.
                </p>

                <p>
                    One of the major reasons of doing this is that it is already setup with James and mine participation in ECESS, and we know how simple it is to setup, so we set it up to improve efficiency. In addition, it allows the team to keep the production code separate from development in any case anything breaks. If the server was a place for development, then if Brian was testing something with the server, it is going to cause roadblocks.  Lastly, Docker images retains the setup on how a program is being built, so we don’t have to worry there is a screw up between our development version compared to our production version.
                </p>

                <p>
                    Brian and I actually struggled with this. We had Web Sockets not being able to transmit data to the server with a snippet of unity code we found online. We didn’t know what was the issue. As a result, we pulled aside and ensure that it worked locally so we can crack down on the issue; long story short, we were not sure at all what the issue was, but after doing a minor change, we were able to get it working. I was worried thinking that it will hopefully worked with <a href="https://ecess-api.matthewwen.com/vrms">https://ecess-api.matthewwen.com/vrms</a> (our api gateway), but nope, it ran perfectly fine because we ensure out development version run as close to what matches in production as possible.
                </p>


                <p class="p-header">Server Setup</p>
                <p>
                    On the server, as mentioned in the previous section, we are using Docker Containers. Here is our current configuration of how our Docker Container runs.
                </p>
                <div style="border: groove;">
                    <code>
                        sudo docker run -d --env-file /home/mwenclubhouse/environments/vrms.env.list \ <br>
                        <span style="margin-left: 50px;"></span>-v /home/mwenclubhouse/vrms:/usr/src/bucket \ <br>
                        <span style="margin-left: 50px;"></span>--name vrms-api \ <br>
                        <span style="margin-left: 50px;"></span>--restart always \ <br>
                        <span style="margin-left: 50px;"></span>-p 2000:8000 \ <br>
                        <span style="margin-left: 50px;"></span>-p 2001:8001 \ <br>
                        <span style="margin-left: 50px;"></span>ghcr.io/477-vrms/vrms-api:main"
                    </code>
                </div>

                <p>
                    We ensure that the container we have running will run on restart, mostly because if Google decides to restart our computer, the program will automatically start up when the computer boots up. We also link port 8000 of the container to port 2000 on the Google VM, and same applies to port 8001 and 2001. Port 8001 is not used for anything at the moment besides
                </p>
                <p>
                    To deal with routing, we are using nginx; we don’t need a load balance, but it is really nice to have nginx deal with routing with our program.
                </p>
                <div style="border: groove;">
                    <code>
                        <span>location /vrms/ { </span><br>
                        <span style="margin-left: 30px;"></span>proxy_pass http://127.0.0.1:2000/; <br>
                        <span style="margin-left: 30px;"></span>proxy_http_version 1.1; <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Upgrade $http_upgrade; <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Connection "Upgrade";  <br>
                        <span style="margin-left: 30px;"></span>proxy_set_header Host $host; <br>
                        } <br>
                    </code>
                </div>

                <p>
                    In this case, we have our docker container running on port 2000 map to
                    <a href="https://ecess-api.matthewwen.com/vrms">https://ecess-api.matthewwen.com/vrms</a>
                    . As a result,
                    <a href="https://ecess-api.matthewwen.com/vrms/route">https://ecess-api.matthewwen.com/vrms/route</a>
                    links to
                    <a href="http://127.0.0.1:2000/route">http://127.0.0.1:2000/route</a> locally on the Google VM. You can see the API is online by going into
                    <a href="https://ecess-api.matthewwen.com/vrms/">https://ecess-api.matthewwen.com/vrms/</a>.
                </p>
                <img src="Files/img/wen101/week2/gateway_demo.png"/>

                <p class="p-header">Programming Language Used for API Application</p>
                <p>
                    So for the backend, we are currently using express-ws, which is a combination of both express as a REST API, and Web-Sockets.
                    <a href="https://github.com/HenningM/express-ws">https://github.com/HenningM/express-ws</a>
                    . Our programming language of choice is Typescript because it performs type checking on top of javascript, so typescript can catch errors before I run into errors. On top of javascript, we are using node to run out javascript code; node is a JIT compiler that converts javascript code into machine code, which then therefore can run on our server.
                </p>

                <p class="p-header">
                    Benchmarking Unit Web Socket
                </p>

                <p>
                    As a software engineering lead, I’m mostly in control on ensuring that all our components can communicate with each other. To reiterate the components is the Unity Software, which runs on the Virtual Reality Headset, the Server, and the IoT device. Using Brian’s Unity code, Brian had an idea to be able to upload joint data from unity to the server. In addition, we also wanted to test how fast it sends data from the server. As a result, I decided to use a Google Firebase Account, where we can use a real time Firebase database so we can easily see the data point changing. The reason on why we decided to Google Firebase because Google created an intuitive software where we can visibly see the changes. In addition, the reason of why we use Google Real Time Database then Google Firestone is because Google Real Time bills us based off the number of bytes sent, not based off of the number of requests, which is another database option offered in Firebase. Keep in mind, we are only using Firebase to check that data is being received, not as an actual service.
                </p>
                <img src="Files/img/wen101/week2/firebase.png"/>
                <p>
                    The structure is not final, as a result, if Brian needs to adjust the data to label them as more specific joints, he can adjust the data that is being posted to the Web Socket.
                    For the Web Socket, the data being sent to the server is in json format. So for this example, it is being sent like this
                </p>

                <div class="div-flex">
                    <div style="border: groove;">
                        <code>
                            { <br>
                            <span style="margin-left: 30px;"></span>"Joint 1": -13.28888, <br>
                            <span style="margin-left: 30px;"></span>"Joint 2": 5.666, <br>
                            <span style="margin-left: 30px;"></span>"Joint 3": 9.42342 <br>
                            }
                        </code>
                    </div>
                    <div style="flex: 1; margin: 5px">
                        <p>
                            At this point of the project, we were not really concern about the data being useful, mostly that the there is a connection between the VR Unity Application and the Server. The server confirms that it obtained data from the VR headset by processing sent via JSON format and then putting it into points→person inside the database. We quickly learn that it updates really fast, even considering the fact on how fast Google Firebase Application runs via Google Servers and on the Computer we are observing the data on. However, we do notice we might be sending data a little too fast, therefore for the later weeks, we need to consider the frequency on how much data to send because sending too much data is just bad and could potentially overload the server and the robot arm badly.
                        </p>
                    </div>

                </div>


                <p class="p-header">What I learned</p>
                <p>
                    Throughout this entire process, I learned about how service accounts work on Google Cloud, and how to connect a Google Cloud Account to a GitHub repository. In addition, I learned how to deploy an API gateway with a Docker Container with the help of using the template from ECE 468
                    <a href="https://github.com/ECE468/env-container-2021-fall">https://github.com/ECE468/env-container-2021-fall</a>
                    , but we did it with node instead of antlr for them. Lastly, I learned how to deploy a web-socket application with my Google Computer (Google VM). In addition, I connected the web-socket application to both the VR headset and a database just to showcase.
                </p>

                <p class="p-header">Next Steps</p>
                <p>The original idea is to use Web Sockets for video streaming since there isn’t really a limit on how much data can be sent in a Web Socket. We also want to implement MQTT, which is also a tcp protocol that should send data faster in smaller packets since the MQTT format has a limit on the amount of data can be sent per packet. We are going to use MQTT for sending data from the unity headset to the robot.</p>

            </div>
            <hr>
            <div id="week3-div">
                <h4>Week 3:</h4>
                <b>Date:</b> Jan 27, 2022<br>
                <b>Total hours:</b> 15 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals of the Week</p>
                <p>
                    My main goal this week was to connect the raspberry pi to the server. This is an important step
                    because we can finally have a proposed pipeline where a unity project (software that runs on the VR
                    headset) can fully connect to the raspberry pi.
                </p>
                <p class="p-header">Setting Up the PI</p>
                <p>
                    The first step is to set up the PI. The raspberry pi will be using Raspbian, the official server
                    side software to run on the PI. We don't need a full GUI to run programs on the PI. The software
                    can be found here <a href="https://www.raspberrypi.com/software/">https://www.raspberrypi.com/software/</a>.
                    In addition, we decided to stick with Python as the software of choice on the PI because of other
                    team member experience with using Python on the PI; I have experience in both C and Python, but using C
                    Programming language could pop up new problems we don't want to run into in the future. At the moment,
                    the PI can successfully turn on inside the lab. The next part is deciding how our program will run
                    on it.
                </p>
                <p class="p-header">Using Systemctl to Run our Software</p>
                <p>
                    We can across <a href="https://en.wikipedia.org/wiki/Systemd">systemd</a>, which is a systemctl
                    client where it can start, stop, and reload programs in the background; you don't need a continuous
                    active terminal in order to run the program. Here is the idea on what we want to run:
                    <code>
                        <br>
                        $ sudo systemctl start vrms-pi # start our program <br>
                        $ sudo systemctl stop vrms-pi # stops our program <br>
                    </code>
                    In addition to being able to start and stop the program, you can also run environment variables etc.
                    This isn't set up yet since I want to write code that can connect to the server first before doing
                    anything else with the pi.
                    <img src="Files/img/wen101/week3/docker-systemd.jpg"/>
                </p>

                <p class="p-header">Setting up Message Broker with the Server and Issues</p>
                <div>
                    We tried using MQTT with our Google Cloud Server, but we realized and the team and TAs came into an
                    agreement to use UDP for most of our protocol instead of TCP. As a result, we decided to change
                    our focus to <a href="https://nodejs.org/api/dgram.html">dgram</a> to create udp websockets. In
                    addition, we initially wanted to use <a href="https://zeromq.org/">ZeroMQ</a> message broker, but
                    it doesn't have UDP support in its primary build; in other words, we have to do additional
                    configuration to enable it. As a result, we are going to use
                    <a href="https://www.rabbitmq.com/">RabbitMQ</a>. So far, I reconfigured the Google Cloud VM
                    to accept UDP traffic into it.
                    <div style="display: flex; margin-top: 10px">
                        <div>
                            <img height="300px" src="Files/img/wen101/week3/gcloud-network-config.jpg"/>
                            <br>
                            <img height="50px" src="Files/img/wen101/week3/gcloud-network-tag.jpg"/>
                        </div>
                        <div style="flex: 1">
                            I did this by first updating the network tag as shown below, which is a rule that states to open
                            all UDP ports.
                            In addition, attach the network tag to our Virtual Google Cloud Machine.
                        </div>
                    </div>
                </div>

                <p class="p-header">What I Learned</p>
                <p>
                    I learned a little about MQTT, which is now useless because we will be switching over to a
                    new type of message broker. In addition, I learned about differences in UDP vs TCP, and which
                    protocol to use; I learned I need to look more deeply into dgram and RTSP for video streaming.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    My next step is to do the connection with the raspberry pi with UDP with the server. We do need
                    to redo the steps for the joint data being sent to the server since that is done using TCP, but
                    we just want to at least have the full pipeline working; we will redo it another time. I need to
                    work close with Emma since she will also be doing a lot of connections between the pi and robotic arm.
                </p>
            </div>

            <div id="week4-div">
                <h4>Week 4:</h4>
                <b>Date:</b> Feb 4, 2022<br>
                <b>Total hours:</b> 20 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Main Goals of the Week</p>
                <p>
                    So my main goal of this week was to start having Brian and Emma communicate how to translate
                    data from unity to data on the arm. It seemed like Brian was moving very quickly with having
                    most of the unity configuration setup, and Emma already has the arm moving. As a result,
                    the next step was to link them where we can demo the arm being moved by Brian using Unity.
                </p>

                <p class="p-header">Reconfiguration of Google Cloud</p>
                <p>
                    So in addition of UDP being added to the Google VM, we also added TCP back into Google Cloud. After
                    looking more into <a href="https://www.rabbitmq.com/">Rabbit MQ</a>, we realized that RabbitMQ requires
                    a RabbitMQ server; we do realize that we can set one up, but our main goal was for us to code the
                    message broker part, so we reverted to <a href="https://zeromq.org/">ZeroMq</a>. We used the
                    <a href="https://github.com/zeromq/zeromq.js/">Node JS Library</a>, as well as the
                    <a href="https://github.com/zeromq/pyzmq">Python Library</a> for the Raspberry Pi. The Raspberry Pi
                    will keep a stable connection with the Google VM, and respond to instances when the server sends an
                    update.
                </p>

                <p class="p-header">Getting Data from Server to Pi</p>
                <p>
                    So we got data onto the pi in a json format. It takes the json from ZeroMQ and parse it into a
                    python object. For now, we put the joint data into a csv file. Here is a small snippet of the data.
                </p>
                <p>Python Program Running on Pi</p>
                <img src="Files/img/wen101/week4/pi-status.jpg"/>
                <p>logs.txt and joint.csv is the output</p>
                <img src="Files/img/wen101/week4/home-directory.jpg"/>
                <p>Content inside of joint.csv</p>
                <div style="overflow: scroll">
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.54'}, 1643926693.8405492, 3.300549268722534 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.61'}, 1643926693.844577, 3.234577178955078 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.67'}, 1643926693.892745, 3.222744941711426 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.75'}, 1643926693.930231, 3.1802310943603516 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.82'}, 1643926694.005648, 3.185647964477539 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.87'}, 1643926694.0465941, 3.1765942573547363 <br>
                    {'J1': -32, 'J2': -6, 'J3': 60, 'J4': 46, 'J5': -50, 'J6': 24, 'J7': 0, 'J8': 0, 'T': '1643926690.90'}, 1643926694.076011, 3.176010847091675 <br>
                </div>
                <p>
                    J1 ... J8 represents the joint data. T represent the epoch time in seconds - the first 4 digits. We kept
                    track on when we received the data, which is the data after the json packet, and then compare the difference.
                    We did this to compare the time it takes to go from unity to the pi, and we realized something seemed off
                    with the data. More will be explained later. However, this is a major success to have data from unity
                    finally show up on the Pi, where Emma can start taking that JSON data and have the arm react to it.
                </p>

                <p class="p-header">What I Learned</p>
                <p>
                    For one our PSSE is to ensure that packets are being sent in less than 0.5 seconds from unity to the
                    pi. We initially had the idea of timestamping the packet with when the packet is created, and then
                    calculating the difference between the epoch time on the pi and the timestamp on the packet.
                    We realized that the clock on both devices are not exactly the same, so we calculated that on
                    average, the time difference is 3 seconds, which is awful. So we used a phone and recorded the change
                    from the unity to the pi, and it was less than 0.5 seconds; we measured the difference in frames
                    by using the iPhone's slo-mo.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    So we got UDP setup, with some socket code on python. It is not running at the moment, but
                    it is on there. Emma got the camera for the pi, so the next step is using udp and sending the video
                    stream data to the server, and then server to website for debugging purposes. We don't know how
                    do it on unity, but we will figure that out on a later date.
                </p>
            </div>

            <div id="week5-div">
                <h4>Week 5:</h4>
                <b>Date:</b> Feb 11, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week, I've been mostly debugging my work because the Raspberry Pi ran into a few issues this week.
                    One of the issue was that it would run out of space because of the continuation of log messages; we
                    were able to fix that. In addition, we still needed to measure the time difference between data sent
                    from the unity headset and data received on the pi. Finally, I was reading up on documentation on
                    the camera we bought, but currently having trouble getting the pi to recognize the camera using the
                    default <a href="https://github.com/raspberrypi/libcamera-apps">libcamera-apps</a> from raspbian
                    themselves.
                </p>

                <p class="p-header">Pi Issues: Camera</p>
                <p>
                    We were able to upload code where the Pi should be able to continuously send data back and forth
                    with the robotic arm; the next step is to do it on command. As a result, I've been helping
                    Emma trying to set that up, so she understands how the data from the server is coming in and out.
                    As in terms of the camera, I'm still confused on how to set that up. My team members found me a
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">tutorial</a>,
                    so I'll use that as reference.
                </p>
                <img src="Files/img/wen101/week5/camera-connected.jpg" style="max-height: 200px"/>
                <img src="Files/img/wen101/week5/lib-camera.jpg" style="max-height: 200px"/>
                <p class="p-header">Pi Issues: SDCard</p>
                <p>
                    In addition, we had an issue with log messages. Since our code doesn't really do anything other than
                    receive data, we just print the data it receives. But it is receiving the data at such a high rate
                    that we ran out of space just to log messages. It was very annoying.
                    As a result, we used this website <a href="https://andreaskaris.github.io/blog/linux/setting-journalctl-limits/">tutorial</a>
                    to limit the amount of log messages to save on the disk. This prevented any errors where we just try to edit a file,
                    or install a new application, but is limited because the log messages took up 11 GB of the disk space.
                </p>
                <img src="Files/img/wen101/week5/fixing-issue.jpg"/>

                <p class="p-header">What I learned</p>
                <p>
                    I learned how to connect the camera to the pi hardware wise; I think I did it correctly because I
                    saw lights turn on, but when I tried to use libcamera to open the camera, it doesn't work. As a
                    result, I have to do some more digging around so then Brian can start playing around with it on
                    unity. According to this
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">website</a>,
                    I have to use VLC player; I'll try it next week.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    Basically getting the camera set up for Brian where the Pi can send data back to the server.
                    After that is complete, I can worry about video streaming data back to unity.
                    Probably going to be watching those PCB videos just learn a little more about PCB, but it
                    wouldn't be my main focus since Emma and James will be working on that mostly.
                </p>
            </div>

            <div id="week6-div">
                <h4>Week 6:</h4>
                <b>Date:</b> Feb 18, 2022<br>
                <b>Total hours:</b> 10 <br>
                <b>Description of design efforts:</b><br>

                <p class="p-header">Lab Synopsis</p>
                <p>
                    This week, I continued working on issues with the Pi, like receiving data from the server.
                    In addition, I worked on getting the camera setup, which is not really working on the
                    moment, more information about that later. Lastly, I was working on the Software Formation Document.
                </p>

                <p class="p-header">Pi Issues: Camera Setup and Getting Data from the Pi</p>
                <p>
                    One issue we ran into this week was not being able to see that the Pi got data from the server. It
                    was working before, but then we added some changes, and it wouldn't budge anymore. As a result,
                    because we used GitHub as part of our file tracking, we backtracked our commits until it began
                    working again; Brian set up unity to send random data, so basically I was looking to see those
                    random data from Brian, and then compared the difference. Apparently, the Pi didn't like the order
                    of declarations inside my background.py file compared to my Macbook.
                </p>
                <img src="Files/img/wen101/week6/recieve-not-working.jpg"/>
                <p>
                    Another issue was the camera again. When following the
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/multi-camera-adapter-board/2mp-stereo-camera-global-shutter-mipi-arducam/#23-display-images-via-vlc-media-player">instructions</a>,
                    from before, it ran into kernel
                    not supported. So instead, I tried to find a version where it did support my kernel version on
                    the Pi; I was able to find
                    <a href="https://www.arducam.com/docs/cameras-for-raspberry-pi/pivariety/how-to-install-kernel-driver-for-pivariety-camera/">this one</a>
                    , and it recognized the camera, but it noted that it was out of date,
                    so nothing will work. After even doing more research, I learned that "out of date" means
                    "not compatible". As a result, I've been trying to build it from source if that has any hope.
                    If this doesn't work, then I'll have to wipe the pi and downgrade it to a lower kernel version.
                </p>

                <p class="p-header">What I learned</p>
                <p>
                    I learned some commands from
                    <a href="https://github.com/raspberrypi/libcamera-apps">libcamera-apps</a>
                    , and then some basics on how to obtain camera
                    data with
                    <a href="https://opencv.org/">opencv</a>
                    , according to documentation of the camera we are using, but I still have a long
                    way to go before actually sending data from the pi to the server.
                </p>

                <p class="p-header">Next Steps</p>
                <p>
                    The next step is still unfortunately connect the camera to the pi and be able to take a picture.
                    Then, be able to send camera data to the server using UDP.
                </p>

            </div>

            <!--            <div id="week3-div">-->
            <!--                <h4>Week 3:</h4>-->
            <!--                <b>Date:</b> Jan 27, 2022<br>-->
            <!--                <b>Total hours:</b> 10 <br>-->
            <!--                <b>Description of design efforts:</b><br>-->
            <!--            </div>-->
            <br>
        </div>

        <!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
        <div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
    $(document).ready(function() {
        $("#header").load("header.html");
        $("#menu").load("navbar.html");
        $("#footer").load("footer.html");
    });
</script>
</body>
</html>
